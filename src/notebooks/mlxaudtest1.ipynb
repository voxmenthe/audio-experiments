{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_audio.tts.generate import generate_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../TEXTS/conv_statsv2.md\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68dd4b7e1e044b6abd6ad0a86847134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[94mModel:\u001b[0m prince-canuma/Kokoro-82M\n",
      "\u001b[94mText:\u001b[0m Okay, I've revised the text, removing the introduction and sprinkling in a few more conversational elements to make it flow even better for listening.\n",
      "\n",
      "Here's the updated version:\n",
      "\n",
      "***\n",
      "\n",
      "**Probability Distributions for Rare Events**\n",
      "\n",
      "Alright, let's dive right in. One of the very first questions we have to tackle in fraud and security is this: How do we actually *model* the occurrence of rare events using statistics? It's a great question! Let's talk about some common statistical distributions that are really useful for representing the counts, frequencies, and even the sizes of these unusual happenings.\n",
      "\n",
      "* **First up, let's consider the Poisson Distribution – this one's fantastic for counts in a fixed interval:** You know, the Poisson distribution is the classic model you reach for when you're counting how many times something happens in a set period, *assuming* these events pop up at a steady average rate. Makes sense, right? It’s often the go-to model for count-based events in fraud detection. For example, imagine a bank typically sees, on average, 2 fraudulent transactions per day. We can model that daily fraud count using a Poisson distribution where the average rate, lambda, is 2. Why does Poisson fit so well? Because it inherently deals with rarity – think about modeling the number of intrusion attempts per hour or counting the phishing emails someone gets each day. We can use it to figure out just how surprising a given count really is. Let's say historically, that lambda is 2 frauds per day. What's the probability of suddenly seeing 6 frauds today? If the Poisson model tells us that probability is extremely low, boom – it flags an anomaly.\n",
      "\n",
      "    Now, how does the math look? The formula for the Poisson probability is basically: the probability of seeing exactly 'k' events equals lambda (the average rate) raised to the power of k, multiplied by 'e' (Euler's number) raised to the power of negative lambda, and then you divide all that by k factorial (which is k times k-1 times k-2... down to 1). When lambda is small – meaning events are rare – small counts like 0, 1, or 2 are the most likely outcomes. But, importantly, the Poisson also tells us the probability of getting those larger counts, which might signal something unusual, like a surge in attacks. If we actually calculated the probability of seeing, say, 10 frauds when the average is only 2, we'd find it's incredibly tiny. That tells us it's highly unlikely to happen just by chance according to this model. So, in a real security system, seeing a count that high could definitely trigger an alert.\n",
      "\n",
      "* **Next, let's talk about the Exponential Distribution – this one's for the time *between* events:** So, while Poisson counts the events, the Exponential distribution focuses on the time *gap* between independent events. Here's a neat connection: if events follow a Poisson process over time, then the time intervals between those events follow an exponential distribution. Makes sense? For example, if a user legitimately logs in randomly about once per hour, the time *between* their logins follows this exponential pattern. Now, what if we suddenly observe two login attempts from that same user just seconds apart? We can use the exponential model to calculate just how improbable that short time gap is. The formula here involves lambda (the rate) times 'e' raised to the power of negative lambda times 't' (time). In fraud detection, exponential models are great for looking at things like the time gap between consecutive transactions or the duration between network connections initiated by a user. Unusually short gaps might signal automated attack scripts, while unusually long gaps could sometimes indicate stalled data exfiltration processes. Think about an insider threat scenario: maybe an employee usually logs in once a day (following an exponential distribution with an average time of 24 hours). If they suddenly log in 10 times within a single hour – that’s a major deviation from their usual pattern!\n",
      "\n",
      "* **And then we have Heavy-Tailed Distributions – these are crucial for handling extreme values:** You see, not everything fits the nice, predictable mold of Poisson or Exponential processes. In cybersecurity and fraud, we often run into phenomena that exhibit \"heavy tails.\" What does that mean? It means that extreme events, although still rare, happen much more often than a standard normal (bell curve) or exponential distribution would predict. A classic example? The size of data breaches. There are tons of small breaches, sure, but there are also occasionally *massive*, headline-grabbing ones. One study actually found that breach sizes often follow something called a log-normal distribution, which has a heavy tail. The researchers pointed out that with these distributions, \"rare events are much more likely to occur than would be expected for normal or exponential distributions.\" So, what's the danger of getting this wrong? If we mistakenly assumed a normal distribution for breach impact, we'd seriously underestimate the probability of those mega-breaches.\n",
      "\n",
      "    Heavy-tailed distributions – like the Pareto, log-normal, or power-law distributions – basically assign more probability to those extreme outcomes way out in the tails. Where else do we see this? Transaction amounts in fraud detection often show a heavy tail: most purchases are modest, but a few are huge (think potentially fraudulent luxury buys). File sizes involved in data exfiltration attempts or the duration of cyber-attacks can also be heavy-tailed. Why is using the right model important? Because it allows us to detect outliers against a more realistic baseline. For instance, if transaction amounts truly follow a Pareto distribution, an extremely large transaction might not be *as* shockingly impossible as if we'd assumed a normal distribution. But, if it falls way out in the far tail, even beyond what the Pareto model predicts, *then* it's a strong red flag.\n",
      "\n",
      "    Let's bring this back to practice. Imagine we're modeling the number of failed login attempts per day coming from different IP addresses. If we use a heavy-tailed distribution (maybe a negative binomial or a power-law), we're explicitly acknowledging that some IPs (like bots in a botnet) are likely to generate *orders of magnitude* more failed attempts than the average user. Our detection thresholds then need to account for this heavy tail, rather than being based on a simple average and standard deviation that gets skewed by these outliers. As another concrete example, research looking at the time intervals *between* network intrusion events found evidence of a \"fat tail\" – meaning longer-than-expected pauses sometimes occurred between attacks, something an exponential model (which assumes events are memoryless) would underestimate. Recognizing these heavy tails helps us avoid false complacency. Instead of dismissing a huge spike as \"so high it must be a data glitch,\" a heavy-tail perspective might say, \"actually, that's statistically possible, though still rare.\" We use the *correct* distribution to quantify just how abnormal an observed event really is.\n",
      "\n",
      "**So, what's the takeaway here?** Choosing the right probability distribution is really the foundation for modeling what's normal versus abnormal in fraud and cyber data. Poisson and exponential distributions are super useful for baseline models of random event counts and timing (like how many login attempts happen, or the time between them). But, heavy-tailed distributions are essential for capturing the reality that extreme losses or sudden attack bursts happen more often than simpler models might suggest. By using these models correctly, we can calculate probabilities or likelihoods for the events we observe and make informed decisions about whether they’re suspiciously improbable (maybe trigger an alert!) or just statistical noise. Next up, let's get formal about making those decisions using hypothesis testing, and crucially, talk about how to keep those false alarms under control.\n",
      "\n",
      "**Hypothesis Testing and Controlling False Positives**\n",
      "\n",
      "Okay, let's think about what happens when we flag something – say, a transaction or a login – as potentially fraudulent or malicious. What are we really doing? We're essentially performing a hypothesis test. We have a default assumption, called the \"null hypothesis\" (or H-zero), which states that \"everything is normal, nothing suspicious here.\" Then we have an \"alternative hypothesis\" (H-one), which says \"hold on, this looks like an anomaly or an attack.\" Statistical hypothesis testing provides a rigorous framework for making this decision, but – and this is a big 'but' in fraud and cyber – it comes with the critical need to carefully manage different types of errors.\n",
      "\n",
      "* **Let's break down those errors: Type I vs Type II:** Ever heard of false positives and false negatives? That's what we're talking about.\n",
      "    * A **Type I error**, or a **false positive**, occurs when we raise an alarm, but it turns out the activity was perfectly benign. Imagine flagging a legitimate customer's perfectly normal purchase as fraud – that's a false positive. Annoying for the customer, right?\n",
      "    * A **Type II error**, or a **false negative**, is the opposite, and often more dangerous: we *fail* to detect an actual malicious event. Think about a fraudulent transaction slipping through completely unnoticed. That's a false negative.\n",
      "    In security speak, a false positive is a false alarm, and a false negative is a missed attack. Finding the right balance between these two is absolutely crucial. Why? Too many false positives, and your security team gets overwhelmed, or worse, they start ignoring the alerts altogether (the 'boy who cried wolf' problem). But allow too many false negatives, and real threats penetrate your defenses.\n",
      "\n",
      "* **Now, let's consider the False Positive Rate and a tricky concept called the Base Rate Fallacy:** This is super important, especially in fraud detection, because the actual rate of fraud (the \"base rate\") is often incredibly low. Maybe only 0.1% of all transactions are truly fraudulent. Here's the counter-intuitive part: because the base rate is so low, even a detection system that seems very accurate on paper can end up generating mostly false alarms!\n",
      "    Imagine an intrusion detection system that boasts a really low false positive rate, say only 0.1%, and a very high true positive rate (it correctly identifies 98% of actual attacks). Sounds great, right? But now, consider that actual attacks might be extremely rare, maybe only 0.01% of all network connections. If you do the math (which involves Bayes' theorem, something we'll touch on later), you'll find that even with these impressive stats, the vast majority of the alarms generated by this system will actually be false positives! A textbook example shows that with 10 million events per day and only 5 actual attacks, a 0.1% false positive rate still leads to about 10,000 false alarms *daily*. Trying to investigate 10,000 false alarms is simply impractical. This really drives home why controlling the *false discovery rate* (the proportion of alarms that are actually false) is paramount. One key strategy is to set extremely strict significance levels for our statistical tests – maybe requiring a p-value smaller than 1 in 100,000 – just to keep the number of false positives down to a manageable level.\n",
      "\n",
      "    * **Need an analogy?** Think about an airport metal detector. The null hypothesis is \"this passenger is not carrying a weapon.\" If the detector is too sensitive, it keeps beeping for things like coins, keys, or belt buckles – those are false positives (Type I errors). If it's not sensitive enough, it might fail to detect an actual weapon – that's a false negative (Type II error). Airport security has to carefully tune the sensitivity (the threshold) so that the alarm primarily rings for genuinely suspicious items. In statistical terms, this often translates to demanding very strong evidence (like a very low p-value) before we conclude something is wrong and raise an alarm.\n",
      "\n",
      "* **So, how does Hypothesis Testing look in Practice for Fraud/Cyber?** We might use standard statistical tests, like a z-test or a chi-square test, to check if something has changed significantly. For example, is the observed fraud rate this week statistically higher than the historical average? The test gives us a *p-value*, which represents the probability of seeing results as extreme as, or more extreme than, what we observed, *if* the null hypothesis (no real change) were true. If this p-value falls below our predetermined threshold (called alpha, often set to something like 0.05, 0.01, or even lower), we reject the null hypothesis and raise an alert. For instance, our hypotheses could be: H-zero: \"Transaction counts are following their usual historical pattern,\" versus H-one: \"Today's transaction counts indicate unusual (possibly fraudulent) activity.\" If we choose an alpha of 0.001, we're accepting a 0.1% chance of making a Type I error (a false alarm) *for that specific test*. But here’s the catch... what happens when we run lots of tests?\n",
      "\n",
      "* **That brings us to the problem of Multiple Testing Correction:** In any real system, we're usually not just running one test. We might be monitoring hundreds of different accounts, or tracking dozens of different metrics simultaneously. If we apply that same 0.001 alpha threshold to each test independently, the overall probability of getting *at least one* false positive across all the tests can become quite high! Think about it: if you run 100 independent tests, each with a 5% chance of a false positive (alpha=0.05), you'd expect to get about 5 false alarms just purely by chance, even if nothing is actually wrong. To combat this inflation of the overall error rate, we need to apply corrections.\n",
      "    * A simple approach is the **Bonferroni correction**: you just divide your desired overall alpha level (say, 0.05 for the whole system) by the total number of tests you're performing. If you're doing 100 tests, your adjusted alpha for each individual test becomes 0.05 / 100 = 0.0005. This is very conservative but effectively controls the family-wise error rate (the probability of making even one false positive).\n",
      "    * Another popular approach focuses on controlling the **False Discovery Rate (FDR)**, which is the expected proportion of false positives among all the tests that you declare significant. Methods like the Benjamini-Hochberg procedure are designed to control the FDR and are generally less conservative (more powerful) than Bonferroni. As one source aptly puts it, \"The larger the number of statistical tests performed, the greater the risk that some of the ‘significant’ findings are due to chance. To protect against false positives... one can set a more stringent threshold... e.g., using the Bonferroni correction\". In cybersecurity, applying these corrections is absolutely essential when you're scanning, say, thousands of network traffic features for anomalies, to avoid being flooded with meaningless alerts.\n",
      "\n",
      "* **Finally, let's not forget Power and controlling Type II errors (False Negatives):** While we're often very focused on minimizing false positives, we absolutely cannot ignore the other side of the coin: the **power** of our test. Power is defined as 1 minus the probability of a Type II error (false negative). In other words, it's the probability that our test *correctly* detects a real anomaly or attack when one is actually present. In fraud detection, missing a genuine fraud (a false negative) can be incredibly costly! There's usually an inherent trade-off: if we make our detection criteria stricter (e.g., lower our alpha threshold) to reduce false positives, we often slightly decrease the power, meaning we might increase the chance of missing some real issues (increase false negatives). Conversely, making the criteria looser increases power but also increases false positives. Where do we set the balance? It really depends on the relative costs of each type of error in the specific context. Banks, for example, might lean towards tolerating a higher number of false positives (which inconvenience some customers) to maximize their chances of catching costly fraudulent transactions. An Intrusion Detection System operating in a very noisy environment, however, might need to be tuned to be less sensitive to reduce the overwhelming \"alert fatigue\" experienced by security analysts. Sometimes, different thresholds are used depending on the context, or even more sophisticated **cost-sensitive learning** approaches are employed, where different costs are explicitly assigned to false positives and false negatives, and the decision threshold is chosen to minimize the total expected cost.\n",
      "\n",
      "**Let's wrap up this section on hypothesis testing:** In the world of fraud and cybersecurity, it's all about making statistically sound decisions regarding what constitutes abnormal behavior, while meticulously managing the associated error rates. The practical workflow usually looks something like this:\n",
      "1.  **Define:** Clearly define a metric or test statistic that effectively captures the type of anomalous behavior you're looking for.\n",
      "2.  **Calculate:** Compute a p-value or an anomaly score for the observed data, based on a statistical model representing normal behavior (the null hypothesis).\n",
      "3.  **Compare:** Compare this p-value or score against a carefully chosen threshold (alpha). This threshold needs to be tuned to achieve an acceptable risk of false positives, which, as we saw, often needs to be very low in these domains.\n",
      "4.  **Correct & Contextualize:** Apply appropriate multiple testing corrections if you're running many tests simultaneously, and always consider domain knowledge and context when interpreting the results.\n",
      "\n",
      "Remember that base rate fallacy calculation? It showed how even a seemingly accurate detector (99% true positive, 1% false positive) could result in over 90% of alarms being false if the actual base rate of attacks is extremely low. This really underscores why we often need very stringent alpha levels (like 0.001 or even 0.0001) or why statistical tests must often be supplemented with additional evidence or rules to achieve a usable level of precision in practice. We'll see later how Bayesian methods offer a natural way to incorporate these base rates directly into the calculation when interpreting alerts.\n",
      "\n",
      "**Time-Series and Sequential Anomaly Detection**\n",
      "\n",
      "Okay, let's shift focus to time. Why? Because many fraud patterns and cyber attacks aren't just single, isolated events – they unfold dynamically over time. Think about it: a single login attempt might look perfectly normal, but a *sequence* of failed attempts from the same IP address could signal a brute-force attack. Similarly, a gradual increase in network traffic over days might indicate slow data exfiltration. So, we need statistical techniques specifically designed for sequential data – data that arrives over time – to help us detect anomalies that have this crucial temporal component.\n",
      "\n",
      "What kind of anomalies are we talking about? Things like a sudden change in transaction frequency, a slow but steady rise in server error rates, or maybe a manufacturing process variable that starts drifting out of spec. Several powerful approaches come from the field of Statistical Process Control (SPC), along with more general change-point detection methods. Let's explore some key ones.\n",
      "\n",
      "* **First, let's look at Control Charts, specifically Shewhart Charts:** These are the workhorses of SPC. Imagine plotting a key metric (like website response time, or number of transactions per hour) on a graph over time. The control chart adds three important lines: a centerline, usually representing the historical average or target value for the metric under normal conditions, and two control limits – an upper control limit (UCL) and a lower control limit (LCL). These limits are often set at, say, plus and minus three standard deviations from the centerline, calculated based on historical data when the process was stable (or \"in control\"). The rule is simple: if a data point falls *outside* the control limits, it's considered a signal that something significant might have changed in the underlying process – a potential anomaly!\n",
      "    These charts were originally developed for monitoring quality in manufacturing, but they're incredibly useful for security monitoring too. You could, for instance, maintain a control chart for the number of failed login attempts per minute. If it normally hovers around a low average, but suddenly spikes above the upper control limit, that's a clear signal possibly indicating a password guessing attack. Or, a bank might chart the total value of wire transfers initiated each day; a point exceeding the UCL could trigger closer scrutiny. Control charts generally assume the data behaves somewhat consistently (often approximated by a normal distribution) when things are stable. Their big advantages are simplicity, ease of implementation, and the great visual insight they provide in real-time. You can literally *see* when a process goes out of bounds.\n",
      "    However, standard Shewhart charts mainly react to individual points crossing the limits. This means they are best at detecting relatively large, sudden shifts. What about smaller, more gradual changes?\n",
      "\n",
      "* **That's where CUSUM (Cumulative Sum) Charts come in:** CUSUM charts are specifically designed to be more sensitive to small, persistent shifts in the process mean that might not cause any single point to fall outside Shewhart limits. How do they achieve this? Instead of just plotting the individual data points, CUSUM charts plot the *cumulative sum* of deviations from a target value. Essentially, at each time point, you calculate how much the current observation deviates from the target (mu-zero), subtract a small \"slack\" value 'k' (which helps ignore insignificant noise and is often set to half the magnitude of the shift you want to detect quickly), add this to the previous cumulative sum, and importantly, reset the sum to zero if it ever drops below zero. This cumulative sum, let's call it S-sub-t, will start to steadily increase if the process mean has shifted upwards (even slightly) and the observations are consistently above the target minus the slack. If this cumulative sum S-sub-t grows beyond a predefined decision threshold 'h', an alarm is triggered.\n",
      "    The intuition is powerful: even small, consistent deviations, which might be missed by a Shewhart chart, will add up over time in the CUSUM calculation, eventually leading to an alarm. This makes CUSUM exceptionally good at catching *slow-burning* problems, like gradually developing fraud schemes or stealthy attacks that unfold over longer periods. For example, imagine an employee starts slowly leaking data. The amount leaked each day might only be slightly above normal, never crossing a 3-sigma limit. But a CUSUM chart tracking the daily leakage volume would accumulate these small positive deviations, and after several days or weeks, it would likely cross its threshold, signaling that a persistent change has occurred. CUSUM is widely recognized in fields like intrusion detection for its efficiency in detecting small shifts in network metrics compared to Shewhart charts.\n",
      "\n",
      "    * **Consider this scenario:** You're monitoring the average CPU utilization on a critical server, which normally runs around 30%. If a piece of stealthy malware starts consuming just a little extra CPU, maybe pushing the average up to 35%, a Shewhart chart might not flag it immediately. But a CUSUM chart tracking the deviation from 30% would likely detect this persistent 5% increase much sooner, as the positive deviations accumulate over time.\n",
      "\n",
      "* **Another related technique is EWMA (Exponentially Weighted Moving Average) Charts:** Like CUSUM, EWMA charts are also designed to be sensitive to smaller shifts, and they achieve this by giving more weight to more recent observations. The EWMA calculates a smoothed value at each time point, Z-sub-t, which is a weighted average of the current observation (x-sub-t) and the previous EWMA value (Z-sub-t-minus-1). The formula is Z-sub-t = lambda * x-sub-t + (1 - lambda) * Z-sub-t-minus-1, where lambda is a smoothing constant between 0 and 1 (smaller lambda means more smoothing, more weight on past data; larger lambda means less smoothing, more weight on the current point). This smoothed EWMA value is then plotted against control limits (which are also adjusted based on lambda). The smoothing helps filter out random noise and makes the chart better at detecting sustained shifts or trends compared to a basic Shewhart chart. In cybersecurity, an EWMA chart might be used to smooth out naturally bursty network traffic patterns to better highlight an underlying trend change, like a slow but steady increase in the rate of connection errors over several hours.\n",
      "\n",
      "* **Finally, let's consider Change-Point Detection Algorithms more broadly:** These are a more general class of statistical techniques specifically aimed at identifying *if and when* the underlying probability distribution generating a sequence of data has changed. Unlike CUSUM or EWMA, which are primarily designed for online monitoring (detecting a change as soon as possible after it occurs), change-point algorithms are often used retrospectively to pinpoint the exact time(s) a change happened in historical data. For instance, analyzing a log of daily transaction volumes over the past year, a change-point algorithm might identify that on March 15th, the average volume significantly increased and stayed high – perhaps indicating the start date of a successful marketing campaign, or maybe something more sinister like the beginning of a coordinated fraud attack.\n",
      "    There are many statistical methods for change-point detection. Some are based on likelihood ratio tests (like the Page-Hinkley test, which is related to CUSUM). Others use Bayesian approaches to estimate the probability of a change point at each time step. Another common strategy involves using a sliding window approach with a two-sample statistical test: you compare the distribution of data points in the most recent window to the distribution in an immediately preceding window (or a fixed baseline window). If a test (like the Kolmogorov-Smirnov test for continuous data, or a chi-square test for categorical data) indicates a statistically significant difference between the two distributions, you declare that a change point likely occurred between the windows.\n",
      "\n",
      "**How might these be applied in Fraud and Cyber scenarios? Let's look at examples:**\n",
      "* A bank could use a **CUSUM chart** monitoring the average transaction amount for each customer, comparing it to their historical baseline. If a customer's account is taken over and the fraudster starts making slightly larger purchases than usual, the CUSUM might catch this gradual shift before a single large transaction occurs.\n",
      "* A Security Operations Center (SOC) could use an **EWMA chart** to track the number of distinct internal hosts contacted by each user's workstation per day. If a user's machine gets compromised and starts scanning the internal network, the EWMA chart tracking this metric might show a smoother, clearer upward trend compared to just plotting the raw daily counts, helping to detect the lateral movement attempt earlier.\n",
      "* **Change-point detection** could be applied to analyze logs of data transfer sizes from a sensitive database server over the past month. If the analysis identifies a specific date and time when the statistical distribution of transfer sizes abruptly changed (e.g., suddenly including many more large transfers), this could pinpoint the start of a major data exfiltration event, allowing investigators to focus their efforts precisely around that time.\n",
      "\n",
      "A simple code simulation could generate, say, 200 data points from a normal distribution with mean 0, followed by another 100 points from a normal distribution with mean 1. Applying a CUSUM procedure to this combined sequence would likely show the cumulative sum starting to steadily increase shortly after point 200, eventually crossing a threshold and signaling the change in mean. This mimics detecting a small but persistent shift in some monitored process.\n",
      "\n",
      "**It's useful to remember the distinction between sequential and batch methods:** Sequential techniques like CUSUM and EWMA are typically designed for *online* monitoring – the goal is to raise an alarm as quickly as possible after a real change occurs. They often involve performing an updated statistical test with each new data point. Change-point detection, on the other hand, is often applied in *batch* mode to analyze historical data and identify all significant change points that occurred within that period. For applications requiring real-time alerting, like flagging fraud as it happens, sequential methods are generally preferred.\n",
      "\n",
      "**So, what's the big picture?** Time-series and sequential anomaly detection techniques allow us to move beyond simply looking at individual data points in isolation. They empower us to detect anomalies that only become apparent when we consider the temporal context – things like unusual bursts of activity, gradual drifts in behavior, or persistent deviations from the norm that simpler methods might miss. These techniques are also fundamental for detecting *concept drift* (which we'll discuss in detail later) by monitoring when the underlying statistical properties of the data stream itself change over time.\n",
      "\n",
      "**Bayesian Reasoning and Probabilistic Modeling**\n",
      "\n",
      "Alright, let's switch gears and explore a different way of thinking about uncertainty: Bayesian statistics. This approach is incredibly powerful for reasoning in situations where information is incomplete or evolving, which is pretty much the standard state of affairs in fraud and cybersecurity detection! Unlike some classical methods that might give a firm yes/no answer based on a single test, Bayesian methods are all about *updating our beliefs* as we gather more evidence. This makes them super useful when trying to figure out, say, the probability that a specific login is malicious, given a whole bunch of different clues.\n",
      "\n",
      "**First things first, let's quickly refresh Bayes' Theorem:** You've probably seen it before. It looks something like this:\n",
      "P(H | E) = [ P(H) * P(E | H) ] / P(E)\n",
      "\n",
      "What does this actually mean?\n",
      "* P(H | E) is the **posterior probability**: What's the probability of our hypothesis H being true, *given* that we've observed evidence E? This is what we want to find out.\n",
      "* P(H) is the **prior probability**: What did we believe about hypothesis H *before* we saw the evidence? This could be based on historical data (like the overall base rate of attacks) or expert knowledge.\n",
      "* P(E | H) is the **likelihood**: If our hypothesis H were true, how likely would we be to observe this evidence E?\n",
      "* P(E) is the **probability of the evidence**: How likely is it to observe evidence E overall (under any hypothesis)? This acts as a normalizing constant.\n",
      "\n",
      "In simple terms, Bayes' theorem tells us how to revise our initial belief (prior) based on new evidence (likelihood) to arrive at an updated belief (posterior). Think of it as: **Updated Belief = Initial Belief × (How well evidence supports belief)**.\n",
      "\n",
      "* **Let's look at a popular application: the Naïve Bayes Classifier:** This is a relatively simple, yet often surprisingly effective, Bayesian model. It's frequently used in fraud detection, especially for classifying text-based things like spam emails or phishing attempts. Why \"Naïve\"? Because it makes a strong simplifying assumption: it assumes that all the different pieces of evidence (features) are *independent* of each other, given the class (e.g., knowing the email contains the word \"free\" doesn't change the probability it also contains \"Viagra\", *if* we already know it's spam). This independence assumption makes the math much easier, as we can just multiply the likelihoods for each piece of evidence together. Now, in reality, features are often *not* perfectly independent, but despite this \"naiveté,\" Naïve Bayes classifiers often perform remarkably well, especially when you have many features (high-dimensional data).\n",
      "    The classic example is **email spam filtering**. Features might be the presence or absence of specific words (\"winner,\" \"free,\" \"money,\" \"click here\"). The filter uses Bayes' theorem to calculate the probability that an email is spam, given the set of words it contains, using historical data about word frequencies in known spam and non-spam emails. As Wikipedia points out, \"Naive Bayes classifiers are a popular statistical technique of e-mail filtering.\"\n",
      "    How about other security uses? **Phishing detection** is a prime candidate (using features from the URL, sender address, email content). **Malware classification** is another (using features like specific system calls made by a program or sequences of instructions). In **fraud detection**, Naïve Bayes could combine evidence like whether the transaction amount is unusual, if the location is new, if the device is unrecognized, etc., into a single overall probability of fraud.\n",
      "\n",
      "    * **Let's walk through a quick example:** Imagine checking credit card transactions using two features: (1) Does the billing ZIP code match the customer's known ZIP? (2) Is the transaction amount unusually high (above $X)? Let's say historical data tells us:\n",
      "        * The prior probability of *any* transaction being fraud, P(F), is very low: 0.1% (or 0.001).\n",
      "        * *Given* a transaction is fraud, the probability of a ZIP mismatch is 50% (0.5), and the probability of a high amount is 70% (0.7).\n",
      "        * *Given* a transaction is legitimate (not fraud), the probability of a ZIP mismatch is only 5% (0.05), and the probability of a high amount is only 10% (0.1).\n",
      "        Now, a new transaction arrives with *both* a ZIP mismatch *and* a high amount. Using the Naïve Bayes assumption (independence of features given the class), we calculate:\n",
      "        * Likelihood of this evidence if it's Fraud: P(E | F) = P(ZIP mismatch | F) * P(High amount | F) = 0.5 * 0.7 = 0.35.\n",
      "        * Likelihood of this evidence if it's Not Fraud: P(E | ¬F) = P(ZIP mismatch | ¬F) * P(High amount | ¬F) = 0.05 * 0.10 = 0.005.\n",
      "        Now we plug these into Bayes' Theorem (along with the prior P(F)=0.001 and P(¬F)=0.999):\n",
      "        P(F | E) = [ P(F) * P(E | F) ] / [ P(F) * P(E | F) + P(¬F) * P(E | ¬F) ]\n",
      "        P(F | E) = [ 0.001 * 0.35 ] / [ (0.001 * 0.35) + (0.999 * 0.005) ]\n",
      "        P(F | E) ≈ 0.00035 / (0.00035 + 0.004995) ≈ 0.065\n",
      "        So, the updated probability of this transaction being fraud, given both signals, is about 6.5%. Notice how this is a huge jump from the initial 0.1% prior, but it's still not a certainty! If more independent evidence pointing towards fraud came in (e.g., unusual time of day), the Naïve Bayes calculation would simply multiply in the likelihood ratio for that new evidence, potentially pushing the probability higher.\n",
      "\n",
      "    Why is Naïve Bayes appealing? It's relatively easy to understand and computationally fast. Plus, it gives you a **probability score**, not just a yes/no answer. This score is gold for risk-based decision making – maybe transactions below 5% probability are allowed, those between 5% and 50% go for manual review, and those above 50% are automatically blocked. While more complex models (like deep learning) might achieve higher accuracy today, Naïve Bayes is often a great starting point or baseline. And the core Bayesian philosophy – updating beliefs based on evidence – underlies many advanced techniques.\n",
      "\n",
      "* **Thinking beyond classification: Bayesian Updating in Incident Response:** Bayesian ideas are also incredibly useful during a live security incident. Imagine an Intrusion Detection System (IDS) that keeps track of suspicious IP addresses. It could maintain a *probability* for each IP, representing the system's current belief that the IP is malicious. This probability starts with a prior (maybe based on threat intelligence feeds or the IP's geolocation). Then, as the IDS observes events linked to that IP – a port scan, an attempt to exploit a vulnerability, communication with a known malicious domain – each event serves as new evidence. The system uses Bayes' theorem to *update* the probability associated with that IP after each observation. This allows the system to build confidence over time. This can be formalized using structures called **Bayesian Networks** (or Belief Networks), where different sensors or detection rules act as evidence nodes influencing the probability of a hypothesis node (like \"Host X is compromised\").\n",
      "\n",
      "* **What about more complex structures? Enter Hierarchical Bayesian Models:** These models are fantastic when your data has some kind of grouped or nested structure. This is super common in fraud detection! Think about transactions grouped by user, or users grouped by the merchant they transact with, or alerts grouped by the security analyst who investigated them. A hierarchical model allows each individual group (like each user) to have its own parameters (e.g., their own typical spending pattern or baseline fraud risk), but it assumes that these individual parameters are themselves drawn from a higher-level population distribution. It's like saying, \"Each user is unique, but users, in general, share some common behavioral tendencies.\" What's the benefit? It allows \"sharing of strength\" or \"borrowing information\" across groups. If you have very little data for one specific user, the model doesn't just rely on that sparse data; it also leverages information from the overall population to make a more stable estimate for that individual. This helps avoid making extreme predictions based on very limited evidence. For example, hierarchical Bayesian methods have been used to group medical providers based on their billing patterns to identify potential fraud, even for providers with relatively few claims. In cybersecurity, you could model each computer in a network as having its own baseline rate of generating certain types of security logs, while having a shared prior distribution reflecting typical behavior across all machines. If one machine starts deviating significantly, the model can flag it more confidently than if it treated each machine in complete isolation.\n",
      "\n",
      "    Another cool application? **Detecting fraud rings.** A hierarchical model could explicitly incorporate a prior belief that certain accounts might be linked (maybe through shared IPs or devices) and potentially part of a coordinated ring. If evidence suggests one account in the suspected ring is fraudulent, the model can use the hierarchical structure to automatically increase the suspicion level (the prior probability) for the other linked accounts.\n",
      "\n",
      "* **And don't forget Bayesian approaches to Sequential Detection:** There are Bayesian versions of the sequential detection techniques we discussed earlier (like Bayesian CUSUM or methods like Shiryaev's procedure). These explicitly incorporate prior probabilities about when or if a change might occur. For instance, in credit card fraud monitoring, you could maintain a continuously updated Bayesian probability representing the belief that a specific card is *currently* being used fraudulently. Each new transaction acts as evidence: normal-looking transactions might slightly decrease this probability, while suspicious ones increase it. If the probability crosses a certain threshold, you take action (like suspending the card). These Bayesian frameworks can be mathematically optimized to find the best trade-off between detection speed and false alarm rate.\n",
      "\n",
      "**So, why embrace the Bayesian way?** A key advantage is that the Bayesian approach provides a natural and principled way to incorporate **prior knowledge** – like the known base rate of fraud – directly into the analysis. Remember that tricky base rate fallacy? Bayesian calculations inherently use the prior probability P(Fraud) when computing the final P(Fraud | Evidence). It also offers an elegant mathematical framework for fusing multiple, potentially uncertain, pieces of evidence. And the output is typically a **probability**, which is often more interpretable and directly usable for risk-based decision making than just a simple alert or score.\n",
      "\n",
      "**What about practicalities?** Naïve Bayes classifiers are indeed very fast and easy to implement, making them great baselines. The main theoretical weakness is the strong independence assumption, but often they work well anyway. If needed, you can try techniques like feature engineering or discretization to reduce dependencies, or move to more complex Bayesian Networks (though that increases complexity). Hierarchical models are computationally more intensive, often requiring specialized methods like MCMC sampling or variational inference, but they can provide very rich insights, especially when dealing with grouped data or wanting to estimate individual-level effects while accounting for population trends. In fast-moving domains like high-frequency trading fraud or real-time intrusion detection, Bayesian *online learning* algorithms, which can continuously update as data streams in, are an active and important area of research.\n",
      "\n",
      "**To sum up the Bayesian perspective:** It equips us with a powerful engine for probabilistic reasoning, especially when dealing with the uncertainty and low base rates typical of fraud and security data. Whether it's a simple Naïve Bayes filter or a sophisticated hierarchical model, the core idea is to **iteratively update our beliefs as new evidence arrives** and to produce interpretable probabilities that guide our decisions. Bayesian methods nicely complement the frequentist approaches (like hypothesis testing) we discussed earlier, and in practice, many systems use a combination of both.\n",
      "\n",
      "Now, let's move on to techniques that are particularly useful when we *don't* have reliable labels or strong prior models – focusing on how to find the \"odd ones out\" directly from the data itself.\n",
      "\n",
      "**Outlier and Novelty Detection**\n",
      "\n",
      "Okay, so far we've talked a lot about modeling known patterns or testing specific hypotheses. But what happens when you don't know exactly what fraud looks like? What if attackers come up with a completely new technique? Often, fraud and security work involves finding things that are simply **novel** or **outliers** – data points that just don't seem to fit the general pattern of the rest of the data, even if we can't label them beforehand. This is where *unsupervised* statistical techniques shine. These methods aim to identify data points that stick out from the crowd without relying on pre-existing labels. Let's explore some key approaches, including robust statistics, distance-based methods, and newer algorithms like Isolation Forest.\n",
      "\n",
      "* **First, let's think about Robust Statistical Measures:** Sometimes, the simplest approaches are quite effective! Instead of using standard measures like the mean and standard deviation – which, as we know, can be heavily thrown off by just one or two extreme values – we can use **robust statistics**. What are those? They are measures designed to be much less sensitive to outliers. Common examples include using the **median** instead of the mean to measure the center of the data, and using the **Median Absolute Deviation (MAD)** or the **interquartile range (IQR)** instead of the standard deviation to measure the spread.\n",
      "    Imagine tracking the number of files accessed by different employees each day. The average (mean) might be skewed high by one person downloading a massive archive. But the median number of files accessed would likely remain stable, representing the typical behavior much better. We could then define an outlier rule based on these robust measures, like: \"Flag any employee accessing more files than the median plus, say, 5 times the MAD.\" This approach extends to multiple dimensions too, with techniques for calculating robust estimates of covariance matrices, which describe the shape and correlations within multivariate data without being distorted by outliers.\n",
      "\n",
      "* **Next up: Mahalanobis Distance for Multivariate Outliers:** When our data has multiple features (like transaction amount, time of day, location data, etc.), simply looking for outliers on each feature independently might miss important cases. An anomaly might involve an *unusual combination* of feature values, where each value on its own isn't necessarily extreme. This is where the **Mahalanobis distance** comes in. Assuming we have a model for the \"normal\" data (often represented by its mean vector $\\mu$ and covariance matrix $\\Sigma$), the Mahalanobis distance tells us how far a given point $x$ is from the center of this distribution, crucially taking into account the correlations between the features captured in $\\Sigma$.\n",
      "    Think of it like measuring distance not with a standard ruler, but with a ruler that's been \"stretched\" or \"rotated\" according to the shape of the data cloud. The formula looks like $d_M(x) = \\sqrt{(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}$. A point far from the mean along a direction where the data is tightly clustered will have a larger Mahalanobis distance than a point equally far along a direction where the data is naturally spread out.\n",
      "    * **Example:** Consider online shopping again, with features 'number of items' and 'total price'. Normally, these are correlated. A transaction with 50 items and a price of $5 might have a huge Mahalanobis distance, even if neither '50 items' nor '$5' is necessarily an outlier on its own, because that *combination* is highly unusual according to the typical correlation structure.\n",
      "    We can flag points with a large Mahalanobis distance as potential anomalies. If we assume the normal data follows a multivariate Gaussian distribution, the *squared* Mahalanobis distance follows a chi-squared ($\\chi^2$) distribution, which gives us a principled way to set a threshold (e.g., flag points whose squared distance exceeds the 99.9th percentile of the relevant $\\chi^2$ distribution).\n",
      "    * **Important Caveat: Robust Covariance!** Just like with the mean and standard deviation, the standard calculation of the covariance matrix ($\\Sigma$) is itself sensitive to outliers in the data used to compute it! If your \"normal\" training data is contaminated with anomalies, the calculated $\\Sigma$ will be distorted, and the resulting Mahalanobis distances might be unreliable. The solution? Use **robust covariance estimation** methods (like the Minimum Covariance Determinant, MCD) that are designed to estimate the covariance structure based only on the \"clean\" core of the data, ignoring the outliers. You'd typically fit this robust $\\mu$ and $\\Sigma$ on a trusted set of normal data, and then use *those* estimates to calculate Mahalanobis distances for new, incoming points.\n",
      "\n",
      "* **Density-Based and Clustering Methods offer another perspective:** Instead of measuring distance from the center, these methods focus on the *local density* of data points. The intuition is that normal points lie in dense regions, while outliers tend to be in sparse, isolated areas.\n",
      "    * **Local Outlier Factor (LOF)** is a popular algorithm here. It calculates a score for each point based on how its local density compares to the local densities of its neighbors. Points in much sparser regions than their neighbors get high LOF scores, indicating they are outliers.\n",
      "    * **One-Class SVM** tries to learn a boundary that encompasses the high-density region where most of the normal data lies. Any point falling outside this boundary is classified as an anomaly or novelty.\n",
      "    * **Clustering algorithms** (like k-means or DBSCAN) can also be repurposed. After clustering the data, you can look for points that don't belong strongly to any cluster, or points that form very small, isolated clusters – these could be considered outliers. Imagine clustering users based on their website navigation patterns; a user whose behavior doesn't fit any of the common patterns might be flagged.\n",
      "\n",
      "* **And then there's Isolation Forest – a clever, modern approach:** This algorithm is specifically designed for anomaly detection and works quite differently. It uses an ensemble of randomized decision trees. How does that help find outliers? The key idea is that **anomalies are typically easier to isolate** than normal points. Think about it: an outlier is usually \"few and different.\" So, if you randomly pick a feature and randomly pick a split point for that feature, an outlier is more likely to end up in a partition by itself (or with very few other points) much quicker than a normal point, which is surrounded by similar points. The Isolation Forest algorithm builds many such random trees. For each data point, it measures the average path length from the root of the tree to the leaf node containing that point, averaged across all trees in the forest. Anomalies, being easier to isolate, will tend to have significantly shorter average path lengths than normal points. This average path length (or a score derived from it) serves as the anomaly score.\n",
      "    Why is Isolation Forest popular, especially in fraud? It tends to work well even with high-dimensional data (many features), it doesn't make strong assumptions about the data distribution (like normality), and it's relatively efficient computationally. You can feed it a diverse set of features describing an event (like a login session: time, location, device, actions taken, etc.), and it will assign anomaly scores indicating which sessions have unusual combinations of these features.\n",
      "\n",
      "    A code example could generate mostly normal data points, plus a few distinct outliers. You'd train an Isolation Forest, perhaps telling it you expect about 2% outliers (the 'contamination' factor). The model would then identify the points with the shortest average isolation paths as the most likely outliers. In real life, you might not know the exact contamination level, but you could use the scores to rank events and investigate the top fraction (e.g., the top 0.1%).\n",
      "\n",
      "**Let's tie this to some specific applications:**\n",
      "* **Catching Novel Credit Card Fraud:** While many systems use supervised learning trained on past fraud, unsupervised methods like Isolation Forest or LOF can run alongside, scanning *all* transactions (even for accounts with no fraud history) to spot highly unusual patterns that might represent *new* fraud techniques the supervised model hasn't seen before.\n",
      "* **Detecting Insider Threats:** You often don't have training data for \"malicious insider behavior.\" Instead, you can use outlier detection. Profile each employee's normal activity (files accessed, systems logged into, work hours). Then, use a method like robust Mahalanobis distance or LOF to detect when an employee's activity significantly deviates from their own established baseline or from their peer group's behavior (e.g., logging in at 3 AM and downloading huge amounts of data they've never touched before).\n",
      "* **Finding Anomalies in Network Traffic:** Unsupervised anomaly detection is a staple in network security monitoring. Models analyze features like traffic volume per protocol, connections per port, packet size distributions, etc. If a rarely used port suddenly experiences a massive surge in connections, it will likely appear as an outlier in this multi-feature space, potentially indicating a scan or an attempt to establish a covert communication channel.\n",
      "\n",
      "* **A Word of Caution on False Positives:** Remember, unsupervised methods are designed to find *statistical* outliers. Not every statistical outlier is necessarily malicious or fraudulent! A sudden surge in sales for a specific product might create transaction patterns that look like outliers but are perfectly legitimate. That's why it's crucial to combine these techniques with domain knowledge and often use them as a *first stage* filter. Anomalies flagged by unsupervised methods often require further investigation or corroboration from other signals before taking action. Tuning the sensitivity (e.g., the contamination factor in Isolation Forest or the distance threshold) is key to balancing detection rates with manageable alert volumes.\n",
      "\n",
      "**So, what's the bottom line on outlier detection?** These unsupervised tools are like your lookout system when you don't know exactly what threat you're searching for. They operate on the principle that bad actors or system failures often manifest as behavior that is statistically rare and significantly different from the norm. The techniques range from simple robust statistics to sophisticated algorithms like Isolation Forest. They are absolutely invaluable for discovering emerging threats (zero-day attacks or new fraud schemes) and providing a crucial safety net that complements supervised detection methods. Once these potential outliers are flagged, understanding *why* they are outliers often involves digging deeper, perhaps by examining the specific features that contributed to their high score, or by using dimensionality reduction techniques – which happens to be our next topic!\n",
      "\n",
      "**Multivariate Techniques for Dimensionality Reduction and Latent Structure**\n",
      "\n",
      "Okay, let's talk about data complexity. As we've seen, data in fraud and cybersecurity often comes with a *lot* of features. A single online transaction might have dozens, even hundreds, of associated data points – amount, time, location, device info, user history, IP address reputation, basket contents... you name it! Dealing with such **high-dimensional data** directly can be really tough. It's hard to visualize, hard to model effectively, and sometimes subtle anomalies get lost in the noise or hidden within complex interactions between all those features. This is where **multivariate statistical techniques** that focus on **dimensionality reduction** and uncovering **latent structure** become incredibly useful. Let's look at two main workhorses: Principal Component Analysis (PCA) and Factor Analysis.\n",
      "\n",
      "* **Let's start with Principal Component Analysis (PCA):** What does PCA do? In essence, it transforms your original set of possibly correlated features into a *new* set of features, called **principal components (PCs)**, which are uncorrelated with each other. It does this in a clever way: the first PC is constructed to capture the maximum possible variance in the original data. The second PC captures the maximum *remaining* variance, subject to being uncorrelated with the first PC, and so on. Why is this useful? If your original features have a lot of redundancy or strong correlations, PCA can often summarize most of the important information (i.e., most of the variance) using just a few principal components – maybe 5 PCs capture 90% of the variance originally spread across 50 features! This gives you a much lower-dimensional representation of your data, which can be easier to work with.\n",
      "\n",
      "    How is this applied in fraud and security?\n",
      "    * **Dimensionality Reduction:** You might take, say, 100 features describing user session behavior and use PCA to reduce them down to maybe 10-15 principal components, which can then be fed into a downstream machine learning model. This can simplify the model, speed up training, and sometimes even improve performance by reducing noise and multicollinearity.\n",
      "    * **Anomaly Detection:** PCA is also widely used directly for anomaly detection. The idea is to build a PCA model using only *normal*, legitimate data. This model essentially defines the \"subspace\" where normal variations occur, spanned by the principal components that capture most of the normal variance. When a new data point arrives, you can project it onto this \"normal subspace\" and then reconstruct it back into the original feature space. The difference between the original point and its reconstruction is the **reconstruction error**. The intuition? Normal points, which follow the patterns learned by PCA, should reconstruct well (low error). Anomalies, which deviate from these normal patterns, will likely reconstruct poorly (high error). So, you can set a threshold on the reconstruction error: points exceeding the threshold are flagged as potential anomalies. This is conceptually similar to using autoencoder neural networks for anomaly detection, but PCA is a linear, more interpretable technique.\n",
      "\n",
      "    * **Think about it:** Imagine PCA learned that normal network traffic mostly varies along axes related to 'overall volume' and 'protocol mix'. If a sudden burst of traffic occurs using a very rare protocol, it won't fit well into this learned normal subspace, resulting in a high reconstruction error when you try to represent it using only the main principal components of normal traffic.\n",
      "\n",
      "* **What about Factor Analysis?** Factor Analysis is related to PCA but comes from a slightly different perspective. While PCA just finds directions of maximum variance, Factor Analysis is based on a *statistical model*. It assumes that the observed features we see are actually generated from a smaller number of underlying, unobserved **latent factors**, plus some random noise specific to each feature. The goal is to identify these latent factors and understand how they influence the observed variables. The classic example is from psychology: scores on various tests (math, verbal, spatial) might all be influenced by an underlying latent factor we call \"general intelligence.\"\n",
      "    In fraud/cyber, Factor Analysis could potentially uncover latent dimensions like \"user risk aversion\" or \"bot-like activity level\" that might explain correlations we observe in user behavior data (e.g., login times, transaction frequencies, mouse movements). While maybe less common than PCA for *direct* anomaly detection, Factor Analysis is valuable for gaining deeper insights into the underlying structure driving the observed data. For instance, if you find latent factors explaining normal user behavior, you could then check how well a specific user's activity aligns with these factors. A user whose behavior isn't well explained by the common latent factors might be considered anomalous. Factor analysis also performs dimensionality reduction, similar to PCA, but its explicit modeling of noise can sometimes be advantageous.\n",
      "\n",
      "* **Using Reduction for Supervised Models:** As mentioned, a very common use case for both PCA and Factor Analysis (and other dimensionality reduction techniques) is as a **preprocessing step** before feeding data into a supervised machine learning model (like a classifier to predict fraud). Reducing the number of features can help combat the \"curse of dimensionality,\" reduce model complexity, prevent overfitting (especially when you have way more features than labeled examples), and handle multicollinearity issues. So, instead of using 50 raw features, your fraud prediction model might work with just the top 10 principal components.\n",
      "\n",
      "* **Visualization Power:** One of the most appealing practical benefits of PCA, especially, is its ability to help us **visualize high-dimensional data**. By plotting the data points based on their scores on the first two (or maybe three) principal components, we can create informative 2D (or 3D) scatterplots. Fraud analysts often rely heavily on these visualizations! Imagine plotting thousands of insurance claims based on their first two principal components. You might see distinct clusters emerge, or you might see known fraudulent claims grouping together in a specific region of the plot. New claims falling into that \"fraudulent region\" become immediate candidates for investigation. Or, you might spot isolated points far away from any cluster, indicating highly unusual claims that also warrant a closer look.\n",
      "\n",
      "* **Keep Limitations in Mind:** It's important to remember that standard PCA is a *linear* technique. It finds linear combinations of features. If the important structures or anomalies in your data are highly nonlinear, PCA might miss them (though extensions like Kernel PCA or using nonlinear methods like autoencoders can help here). Also, PCA's results depend heavily on the data used to fit it. If your training data was biased or contained undetected anomalies, the principal components themselves might not accurately represent true \"normal\" behavior. Factor analysis relies on specific modeling assumptions (like linearity and often Gaussianity) and requires sufficient data to estimate parameters reliably. Despite these limitations, PCA and Factor Analysis are incredibly valuable tools for exploring, simplifying, and finding anomalies in complex multivariate datasets.\n",
      "\n",
      "**So, what's the main takeaway?** Multivariate techniques like PCA and Factor Analysis help us **tame complexity**. They provide ways to reduce the number of features we need to deal with, uncover hidden underlying factors or structures, and often make anomalies stand out more clearly by focusing on deviations from the dominant patterns of variation. They are essential tools for both preprocessing data and for direct exploratory analysis and anomaly detection, acting as a crucial bridge between raw, messy, high-dimensional data and actionable insights.\n",
      "\n",
      "**Graph-Based Statistical Methods for Fraud Rings and Network Intrusions**\n",
      "\n",
      "Okay, let's talk about connections! In fraud and cybersecurity, bad things often don't happen in isolation. Instead, they involve **networks** and **groups** working together. Think about it:\n",
      "* A **fraud ring** might involve multiple fake accounts, all linked by sharing the same address, phone number, or login device.\n",
      "* An **advanced cyber attack** often involves **lateral movement**, where an attacker compromises one machine and then uses it to jump to others within the network, creating a trail of connections.\n",
      "These scenarios scream \"graph!\" We can represent these situations using graphs, where **nodes** are the entities (users, accounts, devices, IPs) and **edges** represent the relationships or interactions between them (transactions, logins, shared info). Why is this useful? Because **graph-based statistical methods** can leverage this relational structure to spot anomalies that methods looking only at individual nodes would completely miss! We can find things like unusually dense clusters (potential fraud rings) or nodes with bizarre connection patterns (like an account suddenly linked to hundreds of others).\n",
      "\n",
      "Let's explore some key graph-based techniques:\n",
      "\n",
      "* **Link Analysis and Following the Trail:** This is a classic investigative technique. If you identify one node as definitely bad (e.g., a confirmed fraudulent account), you can use graph algorithms to automatically find everything connected to it, directly or indirectly. Maybe the fraudulent account shares an address with five other accounts – those five immediately become suspicious. You can extend this outwards: what accounts are linked to *those* five? This \"guilt by association\" propagation is very powerful. It's often combined with probabilistic reasoning (like belief propagation on the graph) to estimate the likelihood that connected nodes are also bad. Even without a known starting point, link analysis tools can identify tightly interconnected clusters that might warrant investigation. Graph databases are perfect for this, allowing queries like \"Find all accounts connected within 2 steps to known fraud account X\" or \"Show me groups of accounts sharing more than 3 attributes.\"\n",
      "\n",
      "* **Community Detection: Unmasking Hidden Groups (like Fraud Rings!):** Imagine a large social network graph. Community detection algorithms aim to find groups of nodes that are much more densely connected *internally* than they are to the rest of the graph. How does this apply to fraud? A **fraud ring** often looks exactly like such a dense community! It might be a set of synthetic identities all transacting heavily with each other to build up fake credit scores, or a group of accounts all controlled by one person using the same few devices. By running algorithms like Louvain or label propagation on a graph linking accounts via shared IPs, devices, addresses, etc., you might discover these tightly knit, potentially illicit communities standing apart from the normal user interactions. As many experts point out, these graph techniques are often more effective because they expose *collective* patterns invisible to single-account analysis.\n",
      "    * **Real-world example:** Graph databases are used to find merchants and cardholders who seem to only transact with each other, forming closed loops – a strong indicator of collusion or money laundering. In cybersecurity, community detection on network traffic might reveal a set of internal computers constantly chatting only among themselves on unusual ports, possibly indicating they're all part of the same botnet.\n",
      "\n",
      "* **Spotting Anomalies in Connections: Degree and Centrality:** Sometimes, just looking at the basic properties of nodes in the graph is revealing.\n",
      "    * **Degree:** Every node has a degree – the number of connections it has. A simple but effective anomaly check is to flag nodes with an extremely high degree (or sometimes, extremely low degree, depending on context). Think about a graph linking users to phone numbers. A single phone number linked to 500 different user accounts? That's highly anomalous and almost certainly points to fraud (like identity verification abuse). It's an outlier in the degree distribution.\n",
      "    * **Centrality:** Measures like *betweenness centrality* identify nodes that act as crucial bridges or hubs connecting different parts of the graph. If an attacker compromises a server, and that server suddenly starts routing traffic between parts of the network that rarely communicated before, its betweenness centrality might skyrocket, signaling its potentially compromised role.\n",
      "\n",
      "* **Finding Suspicious Patterns: Subgraph Mining:** We can also look for specific *patterns* or *motifs* within the graph that are known to be associated with bad behavior. A common example in fraud is finding **dense bipartite subgraphs**: imagine a set of 10 user accounts that are *all* linked to the *same* set of 3 shipping addresses. The probability of this happening randomly is incredibly low; it strongly suggests a single actor controlling those 10 accounts using those 3 drop points. Statistical methods can help quantify just how unlikely a particular observed subgraph pattern is compared to a random baseline model.\n",
      "\n",
      "* **Modeling Network Behavior with Graphs:** In cybersecurity, the network infrastructure itself can be modeled as a graph (hosts are nodes, connections are edges). You can then build statistical models of *normal* communication patterns on this graph. Anomalies could then be detected as:\n",
      "    * **New or disappearing edges:** A server suddenly connecting to an external IP it has never contacted before.\n",
      "    * **Changes in edge weights:** A massive, unexpected surge in traffic volume between two internal servers.\n",
      "    * **Graph-level changes:** A coordinated event, like many internal machines suddenly connecting to the same external command-and-control server (forming a star pattern), would be a graph anomaly detectable by change-point methods applied to graph properties.\n",
      "\n",
      "* **Using Graphs for Prevention:** Graph insights are often used proactively. If a credit card issuer links cards and devices, and card A (linked to device X) is confirmed fraud, they can immediately raise the risk score or block card B, which was also recently seen using device X. This is real-time risk propagation across the graph. Telecoms use call graphs to find cliques indicative of call forwarding fraud. Insurance companies analyze graphs connecting doctors, clinics, and patients to find suspicious referral loops.\n",
      "\n",
      "* **Quantifying Anomaly with Graph Metrics:** We can even define anomaly scores based purely on graph structure. For example, the **local neighborhood structure** around a node can be informative. Does a user's connection pattern (types of links, properties of neighbors) look similar to that of typical users, or is it strangely uniform or connected only to other suspicious nodes? Measures like **graph entropy** or comparing local structure to global patterns (**unexpectedness**) can quantify these deviations.\n",
      "\n",
      "Let's picture that **login graph** again: Nodes are users and IPs, edges mean 'user logged in from IP'. Normal patterns exist (user -> few IPs; IP -> some users). Now, what if we see:\n",
      "1.  An IP node connected to 1000 distinct user accounts? Highly suspicious! Likely a proxy or botnet IP. It's a degree outlier.\n",
      "2.  A group of 50 user accounts *all* connected to the *exact same* 10 IPs (and maybe few others)? This dense bipartite structure screams \"collusion\" or \"single actor control.\" Community detection would likely isolate this group.\n",
      "Graph algorithms excel at quickly traversing these relationships and spotting such collective anomalies. It's no surprise that graph databases and analytics are becoming central tools in modern fraud detection units.\n",
      "\n",
      "**What are the hurdles?** Yes, graph analysis can be computationally demanding on massive networks. Strategies like focusing analysis on specific subgraphs (e.g., within a region, or only recent activity) or using graph sampling techniques can help. Also, remember that not all clusters are malicious! Families sharing addresses, colleagues working on a project – these create legitimate connections. So, graph anomalies often serve as strong *indicators* requiring further validation, perhaps by examining node attributes or combining graph signals with other risk scores.\n",
      "\n",
      "**In a nutshell:** Graph-based methods extend our detection capabilities into the crucial **relational domain**. They are uniquely powerful for uncovering *collective anomalies* (like fraud rings or botnets) and *relational anomalies* (like unexpected connections) that purely node-focused methods would miss. They are indispensable tools for tackling organized fraud, collusion, complex cyber campaigns, and tracking attacker movement within networks. By analyzing the structure of connections, we gain insights that are simply invisible otherwise.\n",
      "\n",
      "**Information-Theoretic Measures for Anomaly Scoring**\n",
      "\n",
      "Let's explore another intriguing way to think about anomalies: using concepts from **information theory**. This field gives us mathematical tools to quantify abstract ideas like \"uncertainty,\" \"randomness,\" and \"surprise\" in data. How can this help in fraud and security? By applying measures like **entropy**, **Kullback-Leibler (KL) divergence**, and **mutual information**, we can detect anomalies by noticing when the *distribution* of data, or the *relationships* between variables, deviates significantly from what we expect. It's like having sensors that detect changes in the information content of the data streams we're monitoring.\n",
      "\n",
      "* **Let's start with Entropy:** You might remember entropy from physics (as a measure of disorder), but in information theory, it measures the **uncertainty** or **randomness** inherent in a probability distribution. High entropy means lots of uncertainty (like rolling a fair die), while low entropy means high predictability (like a loaded die that always lands on 6). Shannon entropy is calculated as $H(P) = -\\sum_i p_i \\log_2 p_i$, where $p_i$ is the probability of the i-th outcome.\n",
      "    How can we use this? By monitoring the entropy of certain features over time!\n",
      "    * **Network Traffic:** Imagine tracking the distribution of destination IP addresses a user connects to. Normally, this might have relatively low entropy (they mostly visit the same few sites). If their machine gets infected with malware that starts scanning random IPs, the distribution of destinations becomes much more uniform, leading to a *spike* in entropy. Conversely, if all machines in a botnet suddenly start contacting the *same* command-and-control server, the entropy of destination IPs might *drop* sharply. Changes in entropy for features like source/destination IPs, ports, or even packet content can be powerful anomaly indicators.\n",
      "    * **User Behavior:** Think about the categories of merchants a credit card holder uses. A normal user might have low entropy (groceries, gas, maybe restaurants). If the card is stolen, the thief might go on a spree across many different, unrelated categories (electronics, jewelry, travel), causing the entropy of the merchant category distribution for that card to suddenly increase.\n",
      "\n",
      "* **Next, Kullback-Leibler (KL) Divergence:** While entropy measures the uncertainty of a *single* distribution, KL divergence measures how different *two* probability distributions are. It quantifies the \"distance\" (though it's not a true symmetric distance) from a distribution P to a reference distribution Q. The formula is $D_{\\text{KL}}(P \\parallel Q) = \\sum_i p_i \\log (p_i / q_i)$. A KL divergence of zero means the distributions are identical; larger values indicate greater divergence.\n",
      "    How is this useful? We can maintain a baseline distribution (Q) representing normal behavior (e.g., the typical distribution of login times, transaction amounts, or API call types). Then, we calculate the distribution (P) for the current time window and compute the KL divergence $D_{\\text{KL}}(P \\parallel Q)$. If this divergence exceeds a threshold, it signals that the current behavior has significantly shifted away from the established norm.\n",
      "    * **Example:** Suppose normally 95% of login attempts are successful, 5% fail. If in the last hour, only 70% were successful and 30% failed, the KL divergence between the current distribution P=[0.7, 0.3] and the baseline Q=[0.95, 0.05] would be significantly greater than zero, indicating a potential problem (like a password spraying attack). Many anomaly detection systems use KL divergence (or related measures) under the hood to detect these kinds of distributional shifts.\n",
      "\n",
      "* **What about Mutual Information (MI)?** Mutual information, $I(X;Y)$, measures the amount of information that one variable (X) provides about another variable (Y). It quantifies the reduction in uncertainty about Y that comes from knowing X. High MI means the variables are strongly related; low MI means they are relatively independent.\n",
      "    How can changes in MI signal anomalies?\n",
      "    * **Broken Links:** Normally, there might be high MI between a user's ID and their primary device ID (knowing the user strongly predicts the device). If accounts start getting taken over, you might see logins for known user IDs coming from many different, unexpected devices, *reducing* the MI between these two variables. A drop in MI could signal this kind of compromise.\n",
      "    * **New Dependencies:** Conversely, an attacker's actions might *create* new dependencies. Maybe a specific malware variant always performs actions A and B in sequence. This creates a correlation (and thus higher MI) between the occurrence of action A and action B, which might not exist in normal user behavior. Monitoring MI between different event types could potentially detect such newly introduced correlations.\n",
      "    MI is also often used in **feature selection** (to find features most informative about fraud) or implicitly in building models like decision trees.\n",
      "\n",
      "* **Other Related Measures:** There are variations! **Jensen-Shannon (JS) divergence** is a symmetric version of KL divergence, useful if you just want to measure the difference between P and Q without a specific direction. **Surprisal** is simply $-\\log P(\\text{event})$, quantifying how unexpected a single event is given your probability model. You could track the average surprisal over time (which is related to entropy) or flag individual events with very high surprisal scores.\n",
      "\n",
      "**Let's see some practical uses:**\n",
      "* **Network Entropy for Worms/Scans:** Tracking the entropy of destination IPs was a classic method for detecting early internet worms and network scans, which caused entropy to spike.\n",
      "* **Entropy for Data Leakage:** Encrypted or compressed data often looks almost random, having very high byte-level entropy. Monitoring the entropy of outgoing data streams can help Data Loss Prevention (DLP) systems flag potential exfiltration of sensitive information disguised as random noise.\n",
      "* **KL Divergence for Behavior Change:** Comparing a user's current distribution of transaction types (or locations, or amounts) to their historical baseline using KL divergence can flag significant deviations that might indicate account takeover or system abuse.\n",
      "* **MI for Linking:** High MI between the activity timing patterns of two supposedly independent accounts could suggest they are controlled by the same entity (sockpuppets).\n",
      "\n",
      "**What are the advantages of these methods?** They are often **model-agnostic** – they don't necessarily require assuming a specific distribution like Gaussian. You can often estimate the needed probabilities non-parametrically from data frequencies. This makes them flexible.\n",
      "**What are the challenges?** Estimating distributions accurately, especially in high dimensions or with limited data, can be tricky. Choosing the right features and the right time granularity (entropy per hour? per user? per IP?) requires careful thought. Often, these information-theoretic measures work best as *indicators*, complementing other techniques. A spike in entropy might trigger a more detailed investigation using outlier or graph methods.\n",
      "\n",
      "**In summary:** Information-theoretic methods offer a unique lens, viewing anomalies as deviations in the \"information content\" or statistical structure of the data. They provide quantitative ways to detect shifts (\"the distribution changed significantly\") or surprises (\"this event was highly unlikely\") that might be missed by simpler thresholding. They are powerful tools in the anomaly detection toolkit, often working behind the scenes in modern detection systems.\n",
      "\n",
      "**Bootstrapping and Resampling for Confidence Estimation**\n",
      "\n",
      "Now, let's talk about confidence. In fraud and security, we're often dealing with messy reality: small numbers of actual fraud cases, noisy data, and models that aren't perfect. When we calculate something – like the current fraud rate, or the threshold for an alert – how much should we trust that number? Is it precise, or could it easily be different if we had slightly different data? This is where **bootstrapping** and related **resampling** techniques become incredibly valuable. They help us estimate the **uncertainty** or **variability** of our statistics *without* having to make strong assumptions about how the data is distributed. Think of it as letting the data itself tell us how much wiggle room there is around our estimates.\n",
      "\n",
      "**So, what exactly *is* Bootstrapping?** It sounds fancy, but the basic idea is wonderfully simple and clever:\n",
      "1.  You take your original dataset (say, a list of transactions from the last month).\n",
      "2.  You create many new \"bootstrap datasets\" by sampling *with replacement* from your original dataset. Each bootstrap dataset has the same size as the original, but some original data points might appear multiple times, and others not at all, due to the replacement.\n",
      "3.  You calculate the statistic you're interested in (e.g., the overall fraud rate, the median transaction amount, the 99th percentile of anomaly scores) for *each* of these bootstrap datasets.\n",
      "4.  You now have a whole distribution of values for your statistic (one value from each bootstrap sample). This distribution approximates the true *sampling distribution* of your statistic – it shows you how much that statistic tends to vary just due to random sampling effects.\n",
      "\n",
      "Why is this so powerful? It allows us to estimate things like confidence intervals or standard errors for virtually *any* statistic, even complex ones, without needing complex mathematical formulas or assuming our data follows a nice textbook distribution (like the normal distribution). It's incredibly versatile!\n",
      "\n",
      "* **Getting Confidence Intervals:** This is a primary use case. Suppose we calculated that 0.2% of transactions last month were fraud. That's our best single estimate, but what's a plausible range? By bootstrapping the transactions (or perhaps daily fraud counts), we get thousands of slightly different estimates of the fraud rate. We can then simply find the range that contains, say, the central 95% of these bootstrap estimates (e.g., the 2.5th percentile and the 97.5th percentile). Voila! That's our 95% bootstrap confidence interval. This is especially useful for rare events (like fraud), where traditional confidence interval formulas often don't work well.\n",
      "    * **Another example:** We tuned an anomaly detection threshold to get a 1% false positive rate on our validation set. But how precise is that threshold? We can bootstrap the *validation set* itself, recalculate the threshold on each bootstrap sample, and get a confidence interval for the threshold value. A wide interval tells us our threshold setting is quite uncertain.\n",
      "\n",
      "* **Assessing Model Robustness:** How sensitive is our machine learning model to the specific data it was trained on? We can use bootstrapping here too. Train the model multiple times, each time using a different bootstrap sample of the original training data. Does the model's performance (e.g., recall on a specific fraud type) vary wildly across these runs? If so, the model might not be very robust. This is the core idea behind ensemble methods like Bagging (Bootstrap Aggregating).\n",
      "\n",
      "* **Dealing with Limited Data:** Imagine you only have 50 examples of a new type of potentially suspicious activity. How do you set an anomaly detection threshold based on just that? You can bootstrap those 50 data points (or their anomaly scores) to get a more stable estimate of, say, the 95th percentile of the scores, along with a confidence interval showing how uncertain that estimate is.\n",
      "\n",
      "* **Related Idea: Permutation Tests:** While bootstrapping resamples data points, permutation tests work by *shuffling labels*. Suppose you want to test if group A behaves differently from group B. You can pool all the data, then repeatedly randomly assign the 'A' and 'B' labels to the data points, recalculating your test statistic each time. This creates a null distribution under the assumption that the labels don't matter, allowing you to see how extreme your original observed difference was. This is great for hypothesis testing without distributional assumptions.\n",
      "\n",
      "A code example could show bootstrapping applied to estimate the mean of data containing a few large outliers. The resulting confidence interval would likely be quite wide, reflecting the uncertainty caused by those outliers – crucial information for risk assessment! Similarly, bootstrapping performance metrics like precision or recall for a fraud model gives confidence intervals, which are much more informative than single point estimates, especially with few fraud cases.\n",
      "\n",
      "**Why bother with all this resampling?** Because it gives us the crucial \"plus or minus\" around our estimates! In high-stakes fields like fraud and security, understanding the *uncertainty* is just as important as the estimate itself. Is the estimated risk 5% +/- 0.1%, or 5% +/- 4%? That makes a huge difference in decision-making! Bootstrapping provides a practical, data-driven way to quantify this uncertainty without relying on potentially unrealistic assumptions.\n",
      "\n",
      "**Key takeaway:** Bootstrapping and resampling are powerful tools for understanding the stability and reliability of our statistical estimates and models, especially when dealing with limited data, rare events, or complex statistics where simple formulas don't apply. They help us put realistic error bars around our numbers.\n",
      "\n",
      "**Concept Drift Detection and Statistical Monitoring of Evolving Attacks**\n",
      "\n",
      "Alright, let's tackle one of the biggest headaches in building long-lasting fraud and security systems: **concept drift**. What is it? Simply put, it's the reality that the world changes! The statistical properties of the data we're monitoring, and even the very definition of what constitutes \"fraud\" or an \"attack,\" can shift over time. Think about it:\n",
      "* **Attackers adapt:** Fraudsters constantly invent new schemes; hackers develop new techniques to bypass defenses. What worked to detect them last year might be useless today.\n",
      "* **Normal behavior changes:** Customers adopt new technologies (mobile banking!), shopping habits change (hello online everything!), economic conditions fluctuate. What looked \"normal\" yesterday might not be the baseline today.\n",
      "\n",
      "Because of this constant evolution, models trained on historical data inevitably become stale and less effective over time. It's a never-ending game of cat and mouse! As one expert aptly put it, \"Fraud detection systems face a constant battle with concept drift... what once flagged fraud may no longer apply\". So, being able to **detect** when drift is happening and **adapt** our systems accordingly is absolutely critical.\n",
      "\n",
      "What kinds of drift do we typically see?\n",
      "* **Covariate Drift:** The distribution of the *input features* changes. Example: A sudden increase in users logging in from a new geographic region due to business expansion. The underlying fraud *mechanisms* might not have changed, but the input data *looks* different.\n",
      "* **Prior Probability Shift (or Target Drift):** The *base rate* of the event changes. Example: The overall percentage of transactions that are fraudulent increases significantly after a major data breach releases millions of credit card numbers.\n",
      "* **Concept/Label Drift:** This is the trickiest one! The *relationship* between the features and the outcome (fraud/not fraud) actually changes. Example: Fraudsters figure out that transactions *under* $50 are rarely scrutinized, so they shift from making large fraudulent purchases to many small ones. The feature \"transaction amount\" now has a different relationship with fraud risk than it did before.\n",
      "\n",
      "**So, how do we spot this drift happening?** We need monitoring! Here are key strategies:\n",
      "\n",
      "1.  **Track Model Performance (if labels are available):** If you have a supervised model (like a fraud classifier) and can eventually get ground truth labels (even with a delay), the most direct way to detect drift is to monitor the model's performance over time. Are metrics like precision, recall, or F1-score degrading? Is the false negative rate creeping up (more missed frauds)? You can even put control charts on these performance metrics! A sustained drop below a lower control limit for recall is a strong drift signal. If labels are scarce or delayed, you might track **proxy metrics**: Is the model becoming less confident (more intermediate scores)? Is the rate of alerts needing manual review increasing?\n",
      "\n",
      "2.  **Monitor Input Data Distributions (Data Drift):** Even without labels, you can (and should!) monitor the statistical properties of the incoming features themselves. Are the average values changing? Are the variances shifting? Are correlations between features changing? You can use:\n",
      "    * **Statistical Tests:** Regularly perform two-sample tests (like KS tests for numerical features, Chi-square for categorical) comparing the distribution of features in recent data to a baseline (e.g., the training data or a known stable period). Significant p-values (after multiple testing correction!) indicate drift.\n",
      "    * **Summary Statistics:** Track simple things like means, medians, standard deviations, min/max values for key features over time. Plot them! Visual inspection can be very revealing.\n",
      "    * **Divergence Measures:** Calculate metrics like Population Stability Index (PSI) or KL/JS divergence between recent and baseline feature distributions. Values exceeding predefined thresholds signal significant drift.\n",
      "\n",
      "3.  **Monitor Model Output Distributions (Prediction Drift):** You can also monitor the distribution of the model's *predictions* on new, unlabeled data. Is the average predicted fraud score suddenly increasing or decreasing? Is the distribution becoming bimodal when it used to be unimodal? Changes here can indicate that the model is \"seeing\" different data, even if you don't know the true labels yet.\n",
      "\n",
      "4.  **Use Dedicated Drift Detection Algorithms:** Researchers have developed algorithms specifically for detecting drift in data streams, often working on model error rates (if labels available) or directly on the data/predictions. Examples include DDM, ADWIN, Page-Hinkley test variants. These often use adaptive windows or statistical tests to signal when a change is statistically significant.\n",
      "\n",
      "5.  **Apply Sequential Change Detection (like CUSUM/EWMA):** We can apply the CUSUM or EWMA charts we discussed earlier not just to raw data, but to derived metrics like the model's average prediction score, the error rate, or even the distribution divergence measures themselves! This helps detect gradual but persistent drifts.\n",
      "\n",
      "**Okay, we detected drift... now what?** Detection is only half the battle! We need to react:\n",
      "\n",
      "* **Retrain/Update the Model:** This is the most common response. Retrain the model using more recent data, possibly weighting recent data more heavily. This allows the model to adapt to the new patterns or distributions.\n",
      "* **Online Learning:** Use models designed to learn incrementally from streaming data. They can potentially track slow drifts automatically (but need careful monitoring to avoid issues like catastrophic forgetting).\n",
      "* **Build Adaptive Ensembles:** Maintain multiple models (e.g., one trained on recent data, one on older data, maybe one specialized for a specific drift type) and combine their predictions, potentially adjusting weights based on which model seems to be performing best currently.\n",
      "* **Investigate and Understand:** Don't just blindly retrain! Try to understand *why* the drift occurred. Was it a new attack pattern? A change in legitimate behavior? A data quality issue? Understanding the cause helps inform the best adaptation strategy. Maybe a rule needs changing, not just the ML model.\n",
      "* **Manual Intervention/Rule Adjustment:** Sometimes drift is predictable or caused by known events (like a holiday season changing spending). Systems might incorporate mechanisms for manual threshold adjustments or temporary rule overrides during such periods.\n",
      "\n",
      "**Think about that chip card example:** When chip cards reduced in-person fraud, criminals shifted massively to online (Card-Not-Present) fraud. This was a major *concept drift*. A model trained before the shift would underestimate the risk of online transactions. Detecting this drift (e.g., by noticing plummeting performance on online transactions) and retraining the model on newer data (where online fraud was prevalent) was essential for the model to remain effective.\n",
      "\n",
      "**What's the practical takeaway?** Concept drift isn't an 'if', it's a 'when'. Building robust fraud and security systems requires building in **continuous monitoring** for drift. This involves tracking data statistics, model performance, and prediction distributions over time, using statistical tests and visualization. When drift is detected, a plan for **adaptation** (usually involving investigation and model updates) must be executed. It's a continuous cycle vital for staying ahead in the adversarial game.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "So, as we wrap up, what's the big picture? We've journeyed through a whole landscape of statistical techniques crucial for tackling fraud and cybersecurity challenges. From foundational probability models like Poisson for understanding rare events, to the rigors of hypothesis testing for making principled alert decisions, and sequential analysis for tracking threats over time – statistics provides the core framework.\n",
      "\n",
      "We saw how Bayesian methods give us a powerful way to update beliefs and combine diverse evidence, while unsupervised outlier detection techniques like Isolation Forest help us find the unknown unknowns – the novel threats. We explored how multivariate methods like PCA can cut through the complexity of high-dimensional data, and how graph analysis unlocks insights into the critical connections between entities, revealing collusion and coordinated attacks. We touched on information theory's role in sensing subtle distributional shifts, and the importance of bootstrapping for understanding the uncertainty in our estimates. And finally, we stressed the absolute necessity of monitoring for and adapting to concept drift in this constantly evolving adversarial landscape.\n",
      "\n",
      "What are the common threads here?\n",
      "* **Dealing with Rarity:** So much of this is about finding needles in haystacks. Statistical tools help quantify rarity and focus on the tails of distributions.\n",
      "* **The Balancing Act:** It's always a trade-off between catching bad actors (minimizing false negatives) and not disrupting legitimate activity (minimizing false positives). Statistics gives us the tools to measure, manage, and make informed decisions about this trade-off.\n",
      "* **Multiple Perspectives:** Effective systems rarely rely on just one technique. They layer multiple approaches – point-in-time analysis, sequential monitoring, individual entity scoring, network analysis – using different statistical tools at each layer.\n",
      "* **Adaptation is Key:** The job is never done. Attackers adapt, so our defenses must too. Continuous statistical monitoring for drift and a readiness to recalibrate and retrain are non-negotiable.\n",
      "\n",
      "Hopefully, by connecting these statistical ideas to concrete examples – control charts catching login spikes, PCA finding weird network traffic, graphs revealing fraud rings – this overview has made it clearer *how* and *why* these tools are applied in practice.\n",
      "\n",
      "If you're working in this field, you'll constantly be blending these techniques, using your statistical knowledge alongside domain expertise about how fraud and attacks actually work. It's that combination that makes this area both incredibly challenging and endlessly fascinating. Keep learning, keep experimenting, and keep using statistical thinking to stay one step ahead!\n",
      "\u001b[94mVoice:\u001b[0m af_heart\n",
      "\u001b[94mSpeed:\u001b[0m 1.2x\n",
      "\u001b[94mLanguage:\u001b[0m a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
      "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:root:Unexpected len(ps) == 511 > 510 and ps == 'ˌɪf jʊɹ tɹˈAnɪŋ dˈATə wʌz bˈIəst ɔɹ kəntˈAnd ˌʌndətˈɛktᵻd ənˈɑməliz, ðə pɹˈɪnsəpᵊl kəmpˈOnənts ðəmsˈɛlvz mˌIt nˌɑt ˈækjəɹətli ɹˌɛpɹəzˈɛnt tɹˈu “nˈɔɹmᵊl” bəhˈAvjəɹ. fˈæktəɹ ənˈæləsɪs ɹᵻlˈIz ˌɔn spəsˈɪfɪk mˈɑdᵊlɪŋ əsˈʌmpʃənz (lˈIk lˌɪniˈɛɹəTi ænd ˈɔfᵊn ɡˌWsɪˈænᵻTi) ænd ɹəkwˈIəɹz səfˈɪʃənt dˈATə tʊ ˈɛstəmˌAt pəɹˈæməTəɹz ɹəlˈIəbli. dəspˈIt ðiz lˌɪmətˈAʃənz, pˌisˌiˈA ænd fˈæktəɹ ənˈæləsɪs ɑɹ ɪnkɹˈɛdəbli vˈæljəwəbᵊl tˈulz fɔɹ ɪksplˈɔɹɪŋ, sˈɪmplᵻfˌIɪŋ, ænd fˈIndɪŋ ənˈɑməliz ɪn kˈɑmplˌɛks mˌʌltivˈɛɹiət dˈATəsˌɛts.'\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate an audiobook chapter as mp3 audio\n",
    "generate_audio(\n",
    "    # text=(\"In the beginning, the universe was created...\\n\"\n",
    "    #     \"...or the simulation was booted up.\"),\n",
    "    text=text,\n",
    "    model_path=\"prince-canuma/Kokoro-82M\", # \"mlx-community/Dia-1.6B\",\n",
    "    voice=\"af_heart\",\n",
    "    speed=1.2,\n",
    "    lang_code=\"a\", # Kokoro: (a)f_heart, or comment out for auto\n",
    "    file_prefix=\"../AUD_FILES/conv_statsv2\", #\"stats_audio\", # audiobook chapter 1\n",
    "    audio_format=\"wav\",\n",
    "    sample_rate=38000, # 24000,\n",
    "    join_audio=True,\n",
    "    verbose=False # True  # Set to False to disable print messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audiobook chapter successfully generated!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Audiobook chapter successfully generated!\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Language Options\n",
    "🇺🇸 'a' - American English\n",
    "🇬🇧 'b' - British English\n",
    "🇯🇵 'j' - Japanese (requires pip install misaki[ja])\n",
    "🇨🇳 'z' - Mandarin Chinese (requires pip install misaki[zh])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43934f89d3542eca91581fac93c11de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Module.load_weights() got an unexpected keyword argument 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[32m      7\u001b[39m model_id = \u001b[33m'\u001b[39m\u001b[33mprince-canuma/Kokoro-82M\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Create a pipeline with American English\u001b[39;00m\n\u001b[32m     11\u001b[39m pipeline = KokoroPipeline(lang_code=\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m, model=model, repo_id=model_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ax/lib/python3.12/site-packages/mlx_audio/tts/utils.py:201\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_path, lazy, strict, **kwargs)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m load_weights_sig.parameters:\n\u001b[32m    199\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m\"\u001b[39m] = strict\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lazy:\n\u001b[32m    204\u001b[39m     mx.eval(model.parameters())\n",
      "\u001b[31mTypeError\u001b[39m: Module.load_weights() got an unexpected keyword argument 'weights'"
     ]
    }
   ],
   "source": [
    "from mlx_audio.tts.models.kokoro import KokoroPipeline\n",
    "from mlx_audio.tts.utils import load_model\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "\n",
    "# Initialize the model\n",
    "model_id = 'prince-canuma/Kokoro-82M'\n",
    "model = load_model(model_id)\n",
    "\n",
    "# Create a pipeline with American English\n",
    "pipeline = KokoroPipeline(lang_code='a', model=model, repo_id=model_id)\n",
    "\n",
    "# Generate audio\n",
    "text = \"The MLX King lives. Let him cook!\"\n",
    "for _, _, audio in pipeline(text, voice='af_heart', speed=1, split_pattern=r'\\n+'):\n",
    "    # Display audio in notebook (if applicable)\n",
    "    display(Audio(data=audio, rate=24000, autoplay=0))\n",
    "\n",
    "    # Save audio to file\n",
    "    sf.write('audio.wav', audio[0], 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audio Experiments",
   "language": "python",
   "name": "ax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
